import os
import random
from re import S

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

class Generator(nn.Module):
    """Generator """
    def __init__(self,class_dim ,in_dim,  hidden_dim, use_cuda):
        super(Generator, self).__init__()
        '''
        self.num_emb = num_emb
        self.emb_dim = emb_dim
        这两个参数用于获得embedding,不需要再考虑这两个参数，直接从文件中读取出来
        '''
        #self.num_emb = num_emb
        self.in_dim = in_dim

        self.emb_dim = 64

        self.hidden_dim = hidden_dim
        self.use_cuda = use_cuda

        self.emb = nn.Sequential(
            nn.Linear(in_features = self.in_dim,out_features = 64),
            nn.ReLU(),
            nn.Linear(in_features = 64,out_features = self.emb_dim),
            nn.ReLU()

        )
        self.output_layer = nn.Sequential(
            nn.Linear(in_features = hidden_dim + self.emb_dim *2 ,out_features = 64),
            nn.ReLU(),
            nn.Linear(in_features = 64 ,out_features = 16),
            nn.ReLU(),
            nn.Linear(in_features = 16,out_features = 1)
            
        )

        self.lstm = nn.LSTM(self.emb_dim, hidden_dim, batch_first=True)
        #self.lin = nn.Linear(hidden_dim + self.emb_dim, 1)
        self.softmax = nn.LogSoftmax(dim = 1)
        self.init_params()

    def forward(self, x,candidates):
        """
        Args:
            x: (batch_size, seq_len), sequence of tokens generated by generator
        """
        emb = self.emb(x)
        #emb 64*21*32 baich_size * seq_len * g_emb_dimision
        h0, c0 = self.init_hidden(x.size(0))
        
        output, (h, c) = self.lstm(emb, (h0, c0))
        #output :baich_size* g_emb_dimision * seq_len
        #output.contiguous().view(-1, self.hidden_dim) 维度 k * 32
        #此处改为k * （32+23）linear输出k*1 应该输出k*4 使用cat，然后计算k*4
        logit = []

        for i in range(candidates.size(0)):
            can = self.emb(candidates[i])
            logit.append(
                self.output_layer(
                torch.cat((output.contiguous().view(-1, self.hidden_dim),can.contiguous().view(-1, self.emb_dim),emb.contiguous().view(-1, self.emb_dim)),
                dim = 1)
                ))
            #logit.append(self.output_layer(torch.cat((torch.cat((output.contiguous().view(-1, self.hidden_dim),can.contiguous().view(-1, self.emb_dim)),dim = 1),emb.contiguous().view(-1, self.emb_dim),dim  = 1)))


        logit = torch.cat(logit,dim = 1)
        pred = self.softmax(logit)
        
        return pred
    

    def init_hidden(self, batch_size):
        h = Variable(torch.zeros((1, batch_size, self.hidden_dim)))
        c = Variable(torch.zeros((1, batch_size, self.hidden_dim)))
        if self.use_cuda:
            h, c = h.cuda(), c.cuda()
        return h, c


    def init_params(self):
        for param in self.parameters():
            param.data.uniform_(-0.05, 0.05)
    
    """
    def emb(x,pdb):
        
        #此函数暂时弃用
        #x: (batch_size, seq_len), sequence of tokens generated by generator
        #pdb_id:(batch_size, seq_len) ,seq from which graph
        #return embedding :(baich_size* g_emb_dimision * seq_len)
        #return adj_embedding
       
        output = Variable(torch.zeros((x.size(0), 23, x.size(0))))
        
        for i in range(x.size(0)):
            pdb_id = pdb[i]
            filepath ="/mnt/data/Storage4/mmy/CryoEM_0112_test/" + pdb_id +"/" +pdb_id+"_ca_feature.txt"
            data = torch.from_numpy(np.load(filepath,dtype = np.float64))
            output[i] = data[x[i],:]
        

        return output

    """

    def step(self,x,h,c,candidates):

        """
        Args:
            x: (batch_size,  1), sequence of tokens generated by generator
            h: (1, batch_size, hidden_dim), lstm hidden state
            c: (1, batch_size, hidden_dim), lstm cell state
        """
        emb = self.emb(x)
        output, (h, c) = self.lstm(emb, (h, c))
        
        scores = torch.zeros((len(candidates)))


        for i in range(candidates.size(0)):
            can = self.emb(candidates[i])
            scores[i] = self.output_layer(torch.cat(
                (output.contiguous().view(-1, self.hidden_dim),
                can.contiguous().view(-1, self.emb_dim),emb.contiguous().view(-1, self.emb_dim)),dim = 1))
             
        
        #for i in range(len(candidates)):

        #    scores[i] = self.lin(torch.cat((output.contiguous().view(-1, self.hidden_dim),candidates[i].contiguous().view(-1, self.emb_dim)),dim = 1))
        
        scores = F.softmax(scores,dim =0)
        
        return scores, h, c
        
    def sample(self,seq_len,edges,features,x = None):
        """
        Args: edges feature 
        此函数有一个bug,即容易陷入环链当中
        
        """
        h, c = self.init_hidden(1)
        samples = []
        if x is None :
            for num in range(100):
                visited = set()
                while(True):
                    start = random.randint(1,len(features)-1)
                    edge  = edges[start-1]
                    if(len(edge)>2):
                        break
                visited.add(start)
                sample = []
                sample.append(features[start])
                x = torch.tensor(features[start]).reshape((1,1, 23))
                x = Variable(x)
                edge  = edges[start-1][1:]
                candidates = []

                for i in range(len(edge)) :
                    candidates.append(features[edge[i]])
                #if self.use_cuda:
                #    x, candidates = x.cuda(), candidates.cuda()
                max_len = 1

                
                for i in range(seq_len):
                    max_len = i
                    candidates = torch.tensor(candidates)
                    candidates = Variable(candidates)
                    if self.use_cuda:
                        x, candidates = x.cuda(), candidates.cuda()
                    score, h, c = self.step(x, h, c,candidates)
                    #score = score[0]
                    if score[0]<1.0 :
                        max_index = torch.argmax(score[1:]) +1
                    else:

                        max_index = torch.argmax(score)
                    if max_index == 0 or edge[max_index] in visited:
                        for j in range(seq_len-i-1):
                            sample.append(features[0])
                        break
                    else:
                        next_node = edge[max_index]
                        visited.add(next_node)
                        edge  = edges[next_node-1][1:]
                        sample.append(features[next_node])
                        x = features[next_node]
                        x = torch.tensor(x)
                        x = Variable(x).unsqueeze(0).unsqueeze(0)
                        candidates = []
                        for i in range(len(edge)):
                            candidates.append(features[edge[i]])
                if(max_len>10):

                    samples.append(sample)

            
        else:
            given_len = x.size(1)
            sample = []
            if self.use_cuda:
                x  = x.cuda()
            
            visited = set()

            for i in range(given_len-1):
                candidates = torch.tensor([])
                candidates = Variable(candidates)
                if self.use_cuda:
                    candidates =  candidates.cuda()

                if x[:,i:i+1,0:1] == -1.0:
                    for j in range(seq_len-i-1):
                        sample.append(features[0])
                    break


                _, h, c = self.step(x[:,i:i+1], h, c,candidates)
                cur = x[:,i:i+1].cpu().numpy().tolist()[0][0]
                sample.append(cur)
                cur = [round(i,3) for i in cur]
                node  = features.index(cur)
                visited.add(node)

            
            x = x[:,-1:]
            #item = sample[-1]
            item = x.cpu().numpy().tolist()[0][0]
            item = [round(i,3) for i in item]
            node  = features.index(item)

            if(len(sample)!=200):
                
                edge  = edges[node][1:]
                candidates = []
                for i in range(len(edge)):
                    candidates.append(features[edge[i]])

                for i in range(given_len-1,seq_len):
                    candidates = torch.tensor(candidates)
                    candidates = Variable(candidates)
                    if self.use_cuda:
                        x,candidates = x.cuda(), candidates.cuda()
                    score, h, c = self.step(x, h, c,candidates)
                    if score[0]<1.0 :
                        max_index = torch.argmax(score[1:]) +1
                    else:

                        max_index = torch.argmax(score)
                    if max_index == 0 or edge[max_index] in visited:
                        for j in range(seq_len-i-1):
                            sample.append(features[0])
                        break
                
                    else:
                        next_node = edge[max_index]
                        visited.add(next_node)
                        edge  = edges[next_node-1][1:]
                        sample.append(features[next_node])
                        x = features[next_node]
                        x = torch.tensor(x)
                        x = Variable(x).unsqueeze(0).unsqueeze(0)
                        candidates = []
                        for i in range(len(edge)):
                            candidates.append(features[edge[i]])

            
                    
            samples.append(sample)
        
        return samples
                